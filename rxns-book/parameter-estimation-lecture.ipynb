{"cells":[{"cell_type":"markdown","metadata":{},"source":["[org-toggle-latex-overlays> ](org-toggle-latex-overlays> )[org-toggle-pretty-entities](org-toggle-pretty-entities)\n\n"]},{"cell_type":"markdown","metadata":{},"source":["\n# Parameter estimation and rate law determination\n\n"]},{"cell_type":"markdown","metadata":{},"source":["-   The rate constants in rate laws generally must be determined from experiments\n\n-   We typically fit models to experimental data, and derive the rate constants from the fitted parameters\n\n-   There are two general types of fitting\n    -   linear regression\n        -   fitting models that are linear in the parameters\n    \n    -   nonlinear regression\n        -   fitting models that are nonlinear in the parameters\n\n-   In either case we want to estimate the value of the parameters in the model, and the uncertainty in the parameters\n\n"]},{"cell_type":"markdown","metadata":{},"source":["\n## Linear regression review\n\n"]},{"cell_type":"markdown","metadata":{},"source":["-   In linear regression we fit a model that is linear in the parameters to some data.\n\n-   The model parameters may be directly useful, e.g. the slope of a line may be related to a rate constant, or we may use the model to derive something, e.g. the derivative at some value.\n\n-   A linear model is one like:\n\n$ y = p_0 + p_1 f_1(x) + p_2 f_2(x) + \\cdots $\n\n-   Here the parameters are $p_i$ and the model is linear in them.\n-   The functions $f_i(x)$ do not have to be linear\n-   Some examples are:\n    -   $ y = p_0 + p_1 x $ - a line\n    -   $ y = p_0 + p_1 x + p_2 x^2 $ - a parabola\n    -   $ y = p_0 + p_1 e^x $\n\n-   We will write these in the general matrix algebra form:\n\n$ \\bf{y} = \\bf{X} \\bf{p} $\n\nwhere:\n\n\\begin{equation}\n\\bf{y} = \\left [\n\\begin{array}{c}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_n\\\\\n\\end{array}\n\\right ]\n\\end{equation}\n\n\\begin{equation}\n\\bf{X} = \\left [ \\begin{array}{cccc}\nf_n(x_1) & \\cdots & f_1(x_1) & 1 \\\\\nf_n(x_2) & \\cdots & f_1(x_2) & 1 \\\\\n\\vdots & \\vdots   & \\vdots   & \\vdots \\\\\nf_n(x_n) & \\cdots & f_1(x_n) & 1 \\\\\n\\end{array}\n\\right ]\n\\end{equation}\n\nand $ \\bf{p} = \\left [\\begin{array}{c}p_n \\\\ p_{n-1} \\\\ \\vdots \\\\ p_0 \\end{array} \\right ]  $\n\n-   The model will usually not fit data perfectly, so we modify the model to include the errors\n\n$\\bf{y} = \\bf{X} \\bf{p} + \\bf{e} $\n\n-   We want the best estimate for **p**, which means we want the  **p** that minimizes the error in the least squares sense (that is the magnitude of the sum of squared errors is minimized)\n\n-   The best estimate for **p** is:\n\n$ \\bf{p} = (\\bf{X}^T\\bf{X})^{-1}\\bf{X}^T \\bf{y} $\n\n-   There will typically be errors between the data and model with corresponding uncertainty in the estimated parameters\n\n-   We need to quantify the uncertainty to determine how important it is in reactor design\n\nLet us consider an example. We want to fit a line to the following data. The line has an equation $y = p_0 x + p_1$.\n\n-   Remember\n\n$ \\bf{p} = (\\bf{X}^T\\bf{X})^{-1}\\bf{X}^T \\bf{y} $\n\nHere we solve a prototypical problem of fitting a line to some data. We are given some x and y data, and we want to fit a line.\n\n[numpy.column_stack](https://docs.scipy.org/doc/numpy/reference/generated/numpy.column_stack.html)\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"The slope is -0.3145221843003413 \nand intercept is 0.000624573378839699\n(2,)\n[-0.31452218  0.00062457]\np [-0.31452218  0.00062457] [-0.31452218  0.00062457]"}],"source":["import numpy as np\nx = np.array([0, 0.5, 1, 1.5, 2.0, 3.0, 4.0, 6.0, 10])\ny = np.array([0, -0.157, -0.315, -0.472, -0.629, -0.942, -1.255, -1.884, -3.147])\n\nX = np.column_stack([x, x**0])\n\n#  I find these intermediate variables make it easier to read\nXTX = np.dot(X.T, X)\nXTy = np.dot(X.T, y)\n\np = np.dot(np.linalg.inv(XTX), XTy)\nslope, intercept = p # note the order in X\nprint('The slope is {0} \\nand intercept is {1}'.format(slope, intercept))\nprint(p.shape)\n# Python 3 syntax\nprint(np.linalg.inv(X.T @ X) @ (X.T @ y))\n\n# plot data and fit\nimport matplotlib.pyplot as plt\nprint('p', p, p.T)\nplt.plot(x, y, 'bo')  # data points\nplt.plot(x, X @ p, 'r--')  # the fit\nplt.xlabel('x')\nplt.ylabel('y')\nplt.savefig('images/la-line-fit.png')"]},{"cell_type":"markdown","metadata":{},"source":["-   You should always plot the fitted function over the data to visually assess the quality of the fit\n\n![img](./images/la-line-fit.png)\n\n\\begin{exercise}\nRedo the last example by defining a function to calculate the summed squared error between the model and data, and use fmin to minimize the summed squared error. Show that you get the same parameters.\n\\end{exercise}\n\n-   The error in the fit is defined as: $\\bf{e} = \\bf{y} - \\bf{X}\\cdot \\bf{p}$\n\n-   We can compute the summed squared error as $SSE = \\bf{e} \\cdot \\bf{e}$\n-   We define $SST = \\sum (\\bf{y} - \\overline{y})^2 = (\\bf{y} - \\overline{y})\\cdot(\\bf{y} - \\overline{y})$\n\n-   We can use that to compute $R^2 = 1 - SSE/SST$ which roughly corresponds to the fraction of variance in the data explained by the model.\n\n-   Let us calculate the R^2 value.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"array([-0.0006, -0.0004, -0.0011, -0.0008, -0.0006,  0.0009,  0.0025,\n        0.0025, -0.0024])\n[-0.0006 -0.0004 -0.0011 -0.0008 -0.0006  0.0009  0.0025  0.0025 -0.0024]\nR-squared = 0.9999972914903201"}],"source":["import numpy as np\nnp.set_printoptions(precision=4)\nimport pprint\n\nx = np.array([0, 0.5, 1, 1.5, 2.0, 3.0, 4.0, 6.0, 10])\ny = np.array([0, -0.157, -0.315, -0.472, -0.629, -0.942, -1.255, -1.884, -3.147])\n\nX = np.column_stack([x, x**0])\n\n#  I find these intermediate variables make it easier to read\nXTX = np.dot(X.T, X)\nXTy = np.dot(X.T, y)\n\np = np.dot(np.linalg.inv(XTX), XTy)\n\ne = y - np.dot(X,p)\npprint.pprint(e)\nprint(e)\n\nSSE = np.dot(e, e)\n\nyb = y - np.mean(y)\nSST = np.dot(yb, yb)\nRsq = 1 - SSE/SST\n\nprint('R-squared = {0} '.format(Rsq))"]},{"cell_type":"markdown","metadata":{},"source":["-   The R^2 tells you how much of the variation in the data is explained by the model.\n    -   a value of 1 tells you all the variation is explained\n    -   values less than one means the model is incomplete in some way\n    -   Here the value is close to one, which suggests a good fit\n\n-   It is important to consider the uncertainty on the parameters\n\n-   pycse has a `regress` function for that\n    -   We specify a confidence level, typically 95%\n    -   &alpha; = (100 - %confidence level)/100\n    -   Let us apply that to the same data set\n\n[pycse.regress](https://www.google.com/#safe=off&q=pycse.regress)\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"The slope is between [-0.31405335 -0.31211207] \nat the 95% confidence level\nThe intercept is between [-0.00275218  0.00056076] \nat the 95% confidence level\n[ -2.42293896e-04  -5.23386903e-05]\n[ -1.47316293e-04  -3.13082708e-01  -1.09570886e-03]\n[  3.69479009e-05   3.77596250e-04   6.44394241e-04]"}],"source":["from pycse import regress\nimport numpy as np\nx = np.array([0, 0.5, 1, 1.5, 2.0, 3.0, 4.0, 6.0, 10])\ny = np.array([0, -0.157, -0.315, -0.472, -0.629, -0.942, -1.255, -1.884, -3.147])\n\nX = np.column_stack([x**2, x, x**0])\n\n# Choose 95% confidence level\nalpha = 1 - 0.95\np, pint, se = regress(X, y, alpha)\ncurvature, slope_interval, intercept_interval = pint\n\nprint('The slope is between {0} \\n'\n      'at the 95% confidence level'.format(slope_interval))\n\nprint('The intercept is between {0} \\n'\n      'at the 95% confidence level'.format(intercept_interval))\n\nprint(curvature)\nprint(p)\nprint(se)"]},{"cell_type":"markdown","metadata":{},"source":["-   Note in this case that the second parameter includes 0\n    -   We cannot conclude that this parameter is significant.\n    -   A simpler model with the intercept fixed at 0 might be better\n\n-   The size of the confidence intervals depends on the number of data points, the number of estimated parameters, and the confidence level.\n\nRead the [regress](https://github.com/jkitchin/pycse/blob/master/pycse/PYCSE.py#L7) source code to learn how the confidence intervals are calculated.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["\n## Applications in determining a rate constant and reaction order\n\n"]},{"cell_type":"markdown","metadata":{},"source":["-   Rate constants and reaction orders are determined by using models that are fit to experimental data\n\n-   A common case is to monitor concentration vs. time in a constant volume, batch reactor\n\n-   We consider the disappearance of $A$\n\n-   From the mole balance we know:\n\n$\\frac{dN_A}{dt} = r_A V$\n\n-   Let us assume the rate law is of the form: $r_A = k C_A^\\alpha$ and a constant volume so that:\n\n$\\frac{dC_A}{dt} = -k C_A^\\alpha $\n\n-   Let us be loose with mathematics, rearrange the equation, and take the log of both sides.\n    -   By loose I mean we take logs of quantities that are not dimensionless\n\n$ \\ln(-\\frac{dC_A}{dt}) = \\ln{k} + \\alpha \\ln C_A $\n\n-   This suggests that if we could numerically compute $\\frac{dC_A}{dt}$ from our data of $C_A(t)$ then a plot of the log of the negative derivative vs the log of concentration would have\n    -   an intercept equal to the log of the rate constant, $k$\n    -   and a slope equal to the reaction order $\\alpha$\n\n-   Given the following data, determine the reaction order in A and the rate constant with 95% confidence intervals.\n\n<table id=\"org7a0e774\" border=\"2\" cellspacing=\"0\" cellpadding=\"6\" rules=\"groups\" frame=\"hsides\">\n\n\n<colgroup>\n<col  class=\"org-right\" />\n\n<col  class=\"org-right\" />\n</colgroup>\n<thead>\n<tr>\n<th scope=\"col\" class=\"org-right\">time (min)</th>\n<th scope=\"col\" class=\"org-right\">C\\_A (mol/L)</th>\n</tr>\n</thead>\n\n<tbody>\n<tr>\n<td class=\"org-right\">0</td>\n<td class=\"org-right\">0.0500</td>\n</tr>\n\n\n<tr>\n<td class=\"org-right\">50</td>\n<td class=\"org-right\">0.0380</td>\n</tr>\n\n\n<tr>\n<td class=\"org-right\">100</td>\n<td class=\"org-right\">0.0306</td>\n</tr>\n\n\n<tr>\n<td class=\"org-right\">150</td>\n<td class=\"org-right\">0.0256</td>\n</tr>\n\n\n<tr>\n<td class=\"org-right\">200</td>\n<td class=\"org-right\">0.0222</td>\n</tr>\n\n\n<tr>\n<td class=\"org-right\">250</td>\n<td class=\"org-right\">0.0195</td>\n</tr>\n\n\n<tr>\n<td class=\"org-right\">300</td>\n<td class=\"org-right\">0.0174</td>\n</tr>\n</tbody>\n</table>\n\n-   We will use the `pycse.deriv` function to numerically compute centered 2-point finite difference approximations to the derivatives\n-   This works best when the $x$ points are evenly spaced, and they should be monotically increasing or decreasing\n\n[pycse.deriv](https://www.google.com/#safe=off&q=pycse.deriv)\n\nRead the [deriv](https://github.com/jkitchin/pycse/blob/master/pycse/PYCSE.py#L182) source code to learn how the derivatives are approximated, and what options are available.\n\n-   Note that we are actually using the data in table [tab-data>](tab-data>)in this code block!\n\n-   We do not have to type the data in ourselves.\n\n-   This causes some false reporting in pyflakes.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"[[0, 0.05], [50, 0.038], [100, 0.0306], [150, 0.0256], [200, 0.0222], [250, 0.0195], [300, 0.0174]]\n<class 'list'>"}],"source":["print(data)\nprint(type(data))"]},{"cell_type":"markdown","metadata":{},"source":["So, we need to convert the list of numbers to a numpy array so we can do the analysis.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"[-2.838  1.79 ]\nalpha = [ 1.482  2.097] at the 95% confidence level\nk = [ 0.019  0.178] at the 95% confidence level"}],"source":["import numpy as np\nnp.set_printoptions(precision=3)  # alternate approach to printing accuracy\nfrom pycse import deriv, regress\nimport matplotlib.pyplot as plt\n\n# data will be a 2d list, which we convert to an array here\ndata = np.array(data)\nt = data[:, 0]   # column 0\nCa = data[:, 1]  # column 1\n\n# calculate numerical derivatives\ndCadt = deriv(t, Ca)\n\n# do the transformation\nx = np.log(Ca)\ny = np.log(-dCadt)\n\n# setup and do the regression\n# column of ones and x:  y = b + mx\nX = np.column_stack([x**0, x])\n\np, pint, se = regress(X, y, 0.05)\n\nintercept_range = pint[0]\nalpha_range = pint[1]\n\nk = np.exp(intercept_range)\nprint(p)\nprint('alpha = {0} at the 95% confidence level'.format(alpha_range))\nprint('k = {0} at the 95% confidence level'.format(k))\n\n# always visually inspect the fit\nplt.plot(x, y,'ko ')\nplt.plot(x, np.dot(X, p))\nplt.xlabel('$\\ln(C_A)$')\nplt.ylabel('$\\ln(-dC_A/dt)$')\nplt.savefig('images/regression-rate.png')"]},{"cell_type":"markdown","metadata":{},"source":["![img](./images/regression-rate.png)\n\n-   You can see there is a reasonably large range of values for the rate constant and reaction order (although the confidence interval does not contain zero)\n\n-   The fit looks ok, but you can see the errors are not exactly random\n    -   There seems to be systematic trends in a sigmoidal shape of the data\n    -   That suggests small inadequacy in the model\n\n-   Let us examine some methods of evaluating the quality of fit\n\n-   First we examine the residuals, or the errors between the data and the model.\n\n-   In a good fit, these will be randomly distributed\n\n-   In a less good fit, there will be trends\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import numpy as np\nnp.set_printoptions(precision=3)\nfrom pycse import deriv, regress\nimport matplotlib.pyplot as plt\n\n# data will be a 2d list, which we convert to an array here\ndata = np.array(data)\nt = data[:, 0]\nCa = data[:, 1]\n\n# calculate numerical derivatives\ndCadt = deriv(t, Ca)\n\n# do the transformation\nx = np.log(Ca)\ny = np.log(-dCadt)\n\n# setup and do the regression\n# column of ones and x:  y = b + mx\nX = np.column_stack([x**0, x])\n\np, pint, se = regress(X, y, 0.05)\n\nresiduals = y - np.dot(X, p)\n\n# always visually inspect the fit\nplt.plot(x, residuals, 'ko-')\nplt.xlabel('$\\ln(C_A)$')\nplt.ylabel('residuals')\nplt.savefig('images/regression-residuals.png')"]},{"cell_type":"markdown","metadata":{},"source":["![img](./images/regression-residuals.png)\n\n-   You can see there are trends in this data\n    -   That means the model may not be complete\n\n-   There is uncertainty in the data\n    -   In each concentration measurement there is uncertainty in the time and value of concentration\n    -   You need more data to reduce the uncertainty\n    -   You may also need better data to reduce the uncertainty\n\n-   Derivatives tend to *magnify* errors in data\n    -   The method we used to fit the data contributed to the uncertainty\n\n-   We also *nonlinearly* transformed the errors by taking logs and exp of the data and results, which may have skewed the confidence limits\n\n"]},{"cell_type":"markdown","metadata":{},"source":["\n### Hybrid methods for data analysis\n\n"]},{"cell_type":"markdown","metadata":{},"source":["-   Numerical differentiation is noisy, but does the least amount of data manipulation, e.g. smoothing\n\n-   Let us consider some hybrid approaches\n\n-   The first hybrid method is to fit a polynomial to the Ca(t) data, and then analytically differentiate the polynomial\n\n-   You must use some judgment about what order polynomial to fit\n    -   Judgment comes from experience\n\n[numpy.polyfit](https://docs.scipy.org/doc/numpy/reference/generated/numpy.polyfit.html)  Fit a polynomial to data\n\n[numpy.polyder](https://docs.scipy.org/doc/numpy/reference/generated/numpy.polyder.html)  Get the derivative of a polynomial\n\n[numpy.polyval](https://docs.scipy.org/doc/numpy/reference/generated/numpy.polyval.html)  Evaluate a polynomial at some data points\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Summed squared error = 3.1308398658445003e-32\nalpha = [ 1.877  2.22 ] at the 95% confidence level\nk = [ 0.078  0.271] at the 95% confidence level"}],"source":["import numpy as np\nnp.set_printoptions(precision=3)\n\nfrom pycse import regress\nimport matplotlib.pyplot as plt\n\n# data will be a 2d list, which we convert to an array here\ndata = np.array(data)\nt = data[:, 0]\nCa = data[:, 1]\n\npCa = np.polyfit(t, Ca, 4)\n\nfCa = np.polyval(pCa, t)\n\nprint('Summed squared error = {}'.format(sum(fCa - Ca)**2))\n\n# always visually inspect the fit\nplt.plot(t, Ca, 'ko ')\nplt.plot(t, fCa)\ntfit = np.linspace(-100, 500)\npfit = np.polyval(pCa, tfit)\nplt.plot(tfit, pfit)\nplt.xlabel('$t$ (min)')\nplt.ylabel('$C_A$ (mol/L)')\nplt.title('Polynomial fit to the data')\nplt.savefig('images/polyfit-1.png')\n\n# [[./images/polyfit-1.png]]\n\n# get the derivative\ndCadt = np.polyval(np.polyder(pCa), t)\n\n# Construct the data we want to fit\n# ln(-dCa/dt) = alpha ln(Ca) + ln(k)\nx = np.log(Ca)\ny = np.log(-dCadt)\n\nX = np.column_stack([x**0, x])\np, pint, se = regress(X, y, 0.05)\n\nintercept_range = pint[0]\nalpha_range = pint[1]\n\nk = np.exp(intercept_range)\n\nprint('alpha = {} at the 95% confidence level'.format(alpha_range))\nprint('k = {0} at the 95% confidence level'.format(k))\n\n# always visually inspect the fit\nplt.figure()\nplt.plot(x, y, 'ko ')\nplt.plot(x, np.dot(X, p))\nplt.xlabel('$\\ln(C_A)$')\nplt.ylabel('$\\ln(-dC_A/dt)$')\nplt.savefig('images/poly-regression-rate.png')"]},{"cell_type":"markdown","metadata":{},"source":["![img](./images/polyfit-1.png)\n\n![img](./images/poly-regression-rate.png)\n\n-   Note the confidence intervals are tighter\n-   That is because the polynomial fitting smooths some of the errors out\n-   We still have nonlinearly transformed errors which may skew the confidence intervals\n\n"]},{"cell_type":"markdown","metadata":{},"source":["\n## Nonlinear regression review\n\n"]},{"cell_type":"markdown","metadata":{},"source":["-   Nonlinear models are abundant in reaction engineering\n    -   $r = k C_A^n $ is linear in the $k$ parameter, and nonlinear in $n$\n\n-   Nonlinear fitting is essentially a non-linear optimization problem\n\n-   Unlike linear regression, where we directly compute the parameters using matrix algebra, we have to provide an initial guess and iterate to the solution\n\n-   Similar to using fsolve, we must define a function of the model\n    -   The function takes an independent variable, and parameters, f(x,a,b,&#x2026;)\n    -   The function should return a value of $y$ for every value of $x$\n    -   i.e. it should be vectorized\n\n-   It is possible to formulate these problems as nonlinear minimization of summed squared errors. See [this example](http://jkitchin.github.io/blog/2013/02/18/Nonlinear-curve-fitting/).\n\n-   The function `scipy.optimize.curve_fit` provides nonlinear fitting of models (functions) to data.\n\n[scipy.optimize.curve_fit](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html)\n\n-   Here is an example usage.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"a = 2.989058049779716 and b=-0.0340876722689279"}],"source":["import numpy as np\nfrom scipy.optimize import curve_fit\n\nx = np.array([0.5, 0.387, 0.24, 0.136, 0.04, 0.011])\ny = np.array([1.255, 1.25, 1.189, 1.124, 0.783, 0.402])\n\n# this is the function we want to fit to our data\ndef func(x, a, b):\n    'nonlinear function in a and b to fit to data'\n    return a * x / (b + x)\n\ninitial_guess = [1, 2]\n\npars, pcov = curve_fit(func, x, y, p0=initial_guess)\n\na,b = pars\nprint('a = {0} and b={1}'.format(a,b))\n\nimport matplotlib.pyplot as plt\nplt.plot(x,y,'bo ')\nxfit = np.linspace(min(x), max(x))\nyfit = func(xfit, *pars)\nplt.plot(xfit,yfit,'b-')\nplt.legend(['data','fit'],loc='best')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.savefig('images/nonlin-curve-fit.png')"]},{"cell_type":"markdown","metadata":{},"source":["![img](./images/nonlin-curve-fit.png)\n\n-   Again, you should always visually inspect the fit\n\nPractice: Repeat this last example by creating a function that calculates the summed squared errors between a model function and the data. Use fmin to find the parameters that minimizes the summed squared error.\n\n-   We also need to estimate uncertainties in nonlinear parameters\n\n-   `pycse` provides a function for this: `nlinfit`.\n\n[pycse.nlinfit](https://www.google.com/#safe=off&q=pycse.nlinfit)\n\nRead the [nlinfit](https://github.com/jkitchin/pycse/blob/master/pycse/PYCSE.py#L53) source code to see how the confidence intervals are computed\n\nHere is an example usage of nlinfit.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"The 95% confidence interval on a is [ 1.301  1.355]\nThe 95% confidence interval on b is [ 0.024  0.029]"}],"source":["import numpy as np\nnp.set_printoptions(precision=3)\nfrom pycse import nlinfit\n\nx = np.array([0.5, 0.387, 0.24, 0.136, 0.04, 0.011])\ny = np.array([1.255, 1.25, 1.189, 1.124, 0.783, 0.402])\n\n\ndef func(x, a, b):\n    'nonlinear function in a and b to fit to data'\n    return a * x / (b + x)\n\ninitial_guess = [1.2, 0.03]\nalpha = 0.05\npars, pint, se = nlinfit(func, x, y, initial_guess, alpha)\n\naint, bint = np.array(pint)\nprint('The 95% confidence interval on a is {0}'.format(aint))\nprint('The 95% confidence interval on b is {0}'.format(bint))"]},{"cell_type":"markdown","metadata":{},"source":["-   Here the two intervals are relatively small, and do not include zero, suggesting both parameters are significant.\n\n-   More importantly, the errors are not skewed by a nonlinear transformation.\n\n-   Note you have to provide an initial guess.\n    -   This will not always be easy to guess.\n    -   There may be more than one minimum in the fit also, so different guesses may give different parameters.\n\n"]}],"metadata":{},"nbformat":4,"nbformat_minor":0}